---
title: "Telemetry analysis clay colored thrush"
author: "Luis Antonio Arias Medellin"
date: "`r Sys.Date()`"
output: html_document
---

## Load required packages

```{r, include=FALSE}
#Load libraries
library("openxlsx")
library("dplyr")
library("NISTunits")
library("rgdal")
library("ggmap")
library("SDMTools")
library("raster")
library("MetaLandSim") # need to run code source("https://bioconductor.org/biocLite.R") biocLite("Biobase")
library("rgeos")
library("maps")
library("maptools")
library("geosphere")
library("tlocoh") #Install using #http://tlocoh.r-forge.r-project.org/manual_install.html instructions and 
#https://r-forge.r-project.org/R/?group_id=1622 ZIP FILE
#Ubuntu
#http://stackoverflow.com/questions/15248815/rgdal-package-installation for rgdal
#You have also to install packages rgeos and pbapply
#http://tlocoh.r-forge.r-project.org/manual_install.html step 5 at bottom of web page for tlocoh
library("car")
library("dplyr")
library("tidyr")
```

## FORMAT DATA BASE FOR ANALYSIS

http://tlocoh.r-forge.r-project.org/tips/isopleth_overlap_auto.html

```{r, include=FALSE}

#Remove anything on workspace
rm(list=ls(all=TRUE))

#Source functions
source("Telemetry_analysis_clay_colored_thrush_functions_a_method_HW.R")
source("Clean telemetry data base.R")

#Fuction for cleaning telemetry data base
clean.tel.data()

#Problems
#312 (10) because points are very separated and do not know if they are ok
#881 (26) 910, 672 (40) 760 (29) 702, 591 (49) error with time stamps
#Missing info from patch 431 (28)


#Done 821 (33) 851 (56) 472 (24) 631 (32) 942 (53) 171 (58) 232 (10) 273 (31)
#511 (34 weird points) 202 (36) 



#Table of frequency of birds that were followed
#table(prueba$FREQ) #24 birds with frequency
#dim(table(prueba$FREQ))
#Mean and sd that birds were followed
sampling<-aggregate(prueba,by=list(prueba$FREQ,prueba$DATE),FUN=length)
sampling<-sampling[order(sampling$Group.1),]
sampling$days<-1
sampling_aggregate<-aggregate(sampling$days,by=list(sampling$Group.1),FUN=sum)
summary(sampling_aggregate[-13,])
sd(sampling_aggregate[-13]$x)

#Number of points obtained per individual
points<-prueba
points$num<-1
points<-aggregate(points$num,by=list(points$FREQ),FUN=sum)
points
#summary(points$x)
sd(subset(points,points$x>1)$x)


hours<-prueba
hours$follow<-difftime(hours$STOP,hours$START,units="hours")
hours<-unique(hours[c("FREQ","DATE","follow")])
hours[order(hours$FREQ),]
hours_sum<-aggregate(hours$follow,by=list(hours$FREQ),FUN=sum)
#summary(as.numeric(hours_sum$x))

#FREQ 821: July 14 nobody sampled a bird
#FREQ 942: July 27 and 29 were sampled by Mau but the information is not in data base
#FREQ 171:  Aug 3 was not sampled but Mau took information and it is not in the data base
#FREQ 973: on July 20 was followed but eaten by a hawk. The radio transmitter was
#recovered and used for other individual followed in August.
#FREQ 392: July 28 was not included and was followed by Mau
#FREQ 273: Aug 5 and 6 was not included and was followed by Tocho and Mau, respectively
#FREQ 472: was followed on Aug 12 but in data base it says Aug 13
#FREQ 511: Aug 9 was sampled by Mau but it is not in the data base
#FREQ 202: followed on Aug 19 and is not in data base but Aug 20 is on data base. Maybe typing mistake
#FREQ 791: followed on Aug 19 and is not in data base but Aug 20 is on data base. Maybe typing mistake
#FREQ 910: followed by Luis but not in data base. In calendar I do not have the date in which I followed it
#FREQ 672: Aug 19 followed by Mau bot it is not in data base
#FREQ 760: Aug 27 and 29 followed by Luis and Mau respectively but no in data base
#Frequ 591: followed Sept 9 but not in data base but Sept 8 is in data base althgough not followed on that day. Maybe typing mistake

#table(format(subset(prueba,prueba$FREQ==591)$DATE,"%B-%d"))
#subset(prueba,prueba$FREQ==821 & prueba$DATE=="2016-07-14")[,1:11]
#subset(prueba,prueba$FREQ==942)[,1:11]

#table(prueba$FREQ,prueba$DATE)

#Merge frequency data with patch id information
#Read excel data base of patch id, patch size and % forest
#freq_patches<-read.xlsx("Frequencies patches.xlsx",sheet=1)
freq_patches<-read.xlsx("Data bases/Frequencies patches.xlsx",sheet=1)

#Merge patches with elevation with frequencies
prueba<-merge(prueba,freq_patches,by="FREQ",all=F)

#
aggregate(PATCH ~ FREQ, data = prueba, FUN = length)


#This part is for knowing the difference between GPS observations taken consecutively
times<-data.frame(times=as.numeric((prueba$MOVE_TIME-prueba$TIME)/60)) 

times<-ifelse(times$times>=0,times$times,NA)
boxplot(times)
hist(times,breaks=15,freq=F)
summary(times)
  
#Remove data bases that I will not use
rm(sampling,points,hours,times)
```

# HW: prepare tables with results

```{r}
#THIS IS ONLY EXECUTED HERE SINCE WE ARE GOING TO USE THE SAME VALUE AND RASTER FOR ALL THE SCRIPT
#Load raster
forest<-raster("Map layers/Aug27_15rasters/forest27")

#****This chunk of code is only executed for the first individual to generate the data frame and then I will use only the functions I have for each specific part****#
#Obtain the radius of the maximum home range. I inputed the value by myself. 
#max_area_buffer<-sqrt(49744.58/pi)
#max_area_buffer<-sqrt(109566.7814/pi)   # HW: largest area
#max_area_buffer<-571.6229                # HW: half of largest a.value

#Obtain the list of frequencies I have
Frequencies<-unique(prueba$FREQ)

# HW: Prepare empty data frame:
areas<-data.frame(matrix(NA, 
                nrow=length(Frequencies), ncol=9, 
                dimnames=list(Frequencies, c("PATCH_ID", "FREQ",
                         "AREA", "a", "n_points",
                         "prop.landscape1", "mean.patch.area1",
                         "prop.landscape2", "mean.patch.area2"))))
```

# HW: Step 1 - Check data

For each frequency, check data and drop observations that are invalid, e.g. duplicates. 

```{r, include=FALSE}
clean.freq()
```

# HW: Step 2 - Get number of points, maximum distance a, and area of 95% homerange using a method

```{r, include=FALSE}
#Convert vector of coordinates to coordinates format
utm_formated<-prueba[,6:7]
utm_formated<-coordinates(utm_formated)
colnames(utm_formated)<-c("x","y")

data.frame(Frequencies)

LHS_data <- LXY_data <- list()
#[-c(10, 13, 16, 19)])
for(i in c(1:length(Frequencies))[-c(14,17,18)]){ #Taking out those frequencies since script stops (see below for solution)
  lxy_data<-xyt.lxy(xy=utm_formated[prueba$FREQ==Frequencies[i],], 
                  dt=prueba$TIME[prueba$FREQ==Frequencies[i]],
                  id=prueba$FREQ[prueba$FREQ==Frequencies[i]], 
                  show.dup.dt=T,
                  proj4string=CRS("+proj=utm +north +zone=17 +ellps=WGS84"))

  #Summary of the lxy object created
  #summary(lxy_data)
  
  areas$n_points[i] <- sum(prueba$FREQ==Frequencies[i])
  areas$FREQ[i] <- Frequencies[i]
  areas$PATCH_ID[i] <- prueba$PATCH[prueba$FREQ==Frequencies[i]][1]
  
  
  areas$a[i] <- a.value <- max(pointDistance(lxy_data[[1]]@coords,lonlat=F))
  #I don't know why I have to add this line to be able to run the next line
  LXY_data[[i]] <- lxy_data <- lxy.nn.add(lxy_data, s=0, a=a.value)
  
  lhs_data <- lxy.lhs(lxy_data,a=a.value,s=0)

  #Add isopleths based on utilization
  LHS_data[[i]] <- lhs_data <-lhs.iso.add(lhs_data)
  
  #Plot each convex hull and save it in a jpg file

  jpeg(paste("Graphs/",Frequencies[i],"gogole_plot.jpg",sep=""),width=1000, height=1000, quality=100)
  par(cex=3)
  plot(lhs_data, iso=T, a=a.value, allpts=T, cex.allpts=1, 
       col.allpts="gray30",ufipt=F)
  scalebar(d=ifelse(areas$a[i]/4>100,round(areas$a[i]/4,digits=-2),round(areas$a[i]/4,digits=-1)),type="bar")
  dev.off()
  #return(plot(lhs_data, iso=T, record=F, ufipt=F))
}

#There is an error in certain frequencies that stops the scripts. For those specific frequencies I added to the xyt.lxy function the part that says tau.diff.max=0. I do not know what it means very well but at least it runs.

for(i in c(1:length(Frequencies))[c(14,17,18)])
{

  lxy_data<-xyt.lxy(xy=utm_formated[prueba$FREQ==Frequencies[i],], 
                  dt=prueba$TIME[prueba$FREQ==Frequencies[i]],
                  id=prueba$FREQ[prueba$FREQ==Frequencies[i]], 
                  show.dup.dt=T,
                  proj4string=CRS("+proj=utm +north +zone=17 +ellps=WGS84"),tau.diff.max=0)

  #Summary of the lxy object created
  #summary(lxy_data)
  
  areas$n_points[i] <- sum(prueba$FREQ==Frequencies[i])
  areas$FREQ[i] <- Frequencies[i]
  areas$PATCH_ID[i] <- prueba$PATCH[prueba$FREQ==Frequencies[i]][1]
  
  areas$a[i] <- a.value <- max(pointDistance(lxy_data[[1]]@coords,lonlat=F))
  #I don't know why I have to add this line to be able to run the next line
  LXY_data[[i]] <- lxy_data <- lxy.nn.add(lxy_data, s=0, a=a.value)
  
  lhs_data <- lxy.lhs(lxy_data,a=a.value,s=0)

  #Add isopleths based on utilization
  LHS_data[[i]] <- lhs_data <-lhs.iso.add(lhs_data)
  
  #Plot each convex hull and save it in a jpg file

  jpeg(paste("Graphs/",Frequencies[i],"gogole_plot.jpg",sep=""),
       width=1000, height=1000, quality=100)
  par(cex=3)
  plot(lhs_data, iso=T, a=a.value, allpts=T, cex.allpts=1, 
       col.allpts="gray30",ufipt=F)
  scalebar(d=ifelse(areas$a[i]/4>100,round(areas$a[i]/4,digits=-2),round(areas$a[i]/4,digits=-1)),type="bar")
  dev.off()
  #return(plot(lhs_data, iso=T, record=F, ufipt=F))
}


areas$n_points
areas$a
```

# HW: Step 3 - Get statistics for buffer

```{r, echo =FALSE}

#Estimate of the maximum radius
max_area_buffer <- max(areas$a / 2, na.rm=TRUE)


isopleths.birds<-list()

for(i in c(1:length(Frequencies)))
{
  #Select hulls and isopleths when a=a.value 
  lhs.a<-lhs.select(LHS_data[[i]], a=areas$a[i])
  isolevels_data<-isopleths(lhs.a)
  ###****
  isopleths.birds<<-append(isopleths.birds,isolevels_data)
  
  areas$AREA[i] <- isolevels_data[[1]]@data$area[5]
  
  #I obtained the centroid from the isolevels list, so it would be for all the isolevels. 
  cen<-gCentroid(isolevels_data[[1]],byid=T)

  #I only need the centroid of the 95% iso level but it has to be as a matrix, so I sustract the value,
  #transform it to a data frame and then to a matrix and then to coordinates
  cen<-data.matrix(data.frame(x=cen@coords[5,1],y=cen@coords[5,2]))
  
  # Identify all cells that lie within buffer around site i:
  Buffer.cells <- raster::extract(forest, cen, cellnumbers=TRUE, 
                        buffer=max_area_buffer)[[1]][,1]

  # Copy land cover map and delete all values outside of buffer:
  Buffer.forest <- forest
  values(Buffer.forest)[-Buffer.cells] <- NA
  
  # just checking! Better: export as jpeg (or png), add points.
  plot(Buffer.forest, ext=c(cen[1]-max_area_buffer,  
                            cen[1]+max_area_buffer,
                            cen[2]-max_area_buffer, 
                            cen[2]+max_area_buffer))
  
  # Calculate class-level metrics for cells within buffer:
  SamplingPoints.class.maxarea <- ClassStat(Buffer.forest,cellsize=10)

  Class <- data.frame(Class.ID=c(1,2))
  Result <- merge(Class, SamplingPoints.class.maxarea, 
                  all=TRUE, by.x="Class.ID", by.y="class")
  Result[is.na(Result)] <- 0

  #Obtain statistics of buffer equal to the area of the largest home range
  samplingPoints.class.homerange<-Result[,c(1,4,10)]

  areas[i, c(6:9)] <- c(samplingPoints.class.homerange[1,2:3],
                        samplingPoints.class.homerange[2,2:3])
  cat(Frequencies[i], " ")
}

areas

#save(isopleths.birds, file="isopleths.birds.RData")
```


# Data analysis of home range size vs patch size and proportion of forest

```{r,warning=F,message=F,include=F}

#Merge areas with elevation information of the patch
areas<-merge(areas,prueba<-read.xlsx("Data bases/Information patches Costa Rica.xlsx",sheet=1,cols=c(1,5)))

#Read data base
#areas<-read.csv("areas database.csv")

#Know statistics about the data base
#Mean home range size (dividing by 10,000 so that I can have the results in hectares)
summary(areas$AREA/10000)
sd(areas$AREA/10000)
boxplot(areas$AREA)
hist(areas$AREA/10000)

#Statistics taking out two outlayers with large home ranges
boxplot(subset(areas,areas$AREA<80000)$AREA/10000)
hist(subset(areas,areas$AREA<80000)$AREA/10000)
summary(subset(areas,areas$AREA<80000)$AREA/10000)

#Maximum value of a
max(areas$a)

areas[order(areas$prop.landscape2),c(1:3,8)]



panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
```

```{r}
#Areas in hectares
areas$AREA<-areas$AREA/10000

pairs(areas[c("AREA","a","n_points","prop.landscape2","mean.patch.area2","ELEVATION")],upper.panel=panel.smooth,diag.panel=panel.hist,panel=panel.cor,main="Raw data")

#qqplot of variables
qqPlot(areas$prop.landscape2)
qqPlot(areas$ELEVATION)

#Make plot with home range area log transformed
areas$logAREA<-log(areas$AREA)
areas$logmean.patch.area2<-log(areas$mean.patch.area2)

pairs(areas[c("logAREA","a","n_points","prop.landscape2","logmean.patch.area2","ELEVATION")],upper.panel=panel.smooth,diag.panel=panel.hist,panel=panel.cor,main="Log of area, proportion of forest and patch size")

```

```{r}
#######GLMM#####
library("lme4")
library("nlme")
#Converting random variables to factor
areas$PATCH_ID<-as.factor(areas$PATCH_ID)
#areas$FREQ<-as.factor(areas$FREQ)
library("usdm")
vif(data.frame(areas$logAREA,areas$asinsqrt.prop.landscape2,areas$logmean.patch.area2,areas$ELEVATION))


#Model with random intercept. You have to use ML instead of REML to be able to compare AIC. I am using the logarithm in some variables so that they are normally distributed
#Create a data frame with scaled variables
areas.scaled<- areas %>%
  mutate(log.AREA=log(AREA/10000),
         log.mean.patch.area2=scale(log(mean.patch.area2)),
         ELEVATION=scale(ELEVATION))

#Full model
summary(random_mod_1<-lme(log.AREA ~ prop.landscape2 + log.mean.patch.area2 + ELEVATION, random = ~1|PATCH_ID,method = "ML", data = areas.scaled))

#Without mean patch area
summary(random_mod_2<-lme(log.AREA ~ prop.landscape2  + ELEVATION, random = ~1|PATCH_ID,method = "ML", data = areas.scaled))


#Select best random factor model using AIC
models.aic<-AIC(random_mod_1,random_mod_2)
library("MuMIn")
round(Weights(models.aic),digits=2) #Best model #2. 

#Run the best model with REML
random_mod_2<-lme(log.AREA ~ prop.landscape2  + ELEVATION, random = ~1|PATCH_ID,method = "REML", data = areas.scaled)

#This is the best model, I will assess which variables I should retain
summary(random_mod_2) #Proportion of forest was singificant

#Obtain r squared of model
r.squaredGLMM(lmer(log.AREA ~ prop.landscape2  + ELEVATION + (1|PATCH_ID), data = areas.scaled))

#Graph variables that were singificant
areas %>% 
  ggplot(aes(x=prop.landscape2,y=logAREA)) +
  geom_point() + 
  geom_smooth(method="lm")

areas %>% 
  ggplot(aes(x=prop.landscape2,y=ELEVATION)) +
  geom_point() + 
  geom_smooth(method="lm")


#Checking assumptions. Extracting the fitted and residual values
residuals.mod<-resid(random_mod_2,type="normalized")
fitted.mod<-fitted(random_mod_2)

#Plot
plot(fitted.mod,residuals.mod)
plot(areas.scaled$prop.landscape2,residuals.mod)
plot(areas.scaled$ELEVATION,residuals.mod)

#The residuals look normaly distributed
qqPlot(random_mod_2$residuals)

#Checking homogeneity of variance. SEEMS VERY GOOD
plot(random_mod_2)

```

```{r}
#Run the best model with REML and hectares
summary(random_mod_2<-lme(logAREA ~ prop.landscape2  + ELEVATION, random = ~1|PATCH_ID,method = "REML", data = areas))

#MAKE PREDICITION. I DID IT MANUALLY BUT DID NOT INCLUDE TERMS FOR RANDOM FACTOR
areas<-areas %>% 
  mutate(prediction=random_mod_2$coefficients$fixed[1] + prop.landscape2*random_mod_2$coefficients$fixed[2] + ELEVATION*random_mod_2$coefficients$fixed[3])

#Plot predicted values
jpeg("Graphs/Home range size vs prop forest.jpg",quality=100,width=1000,height=700)
par(mar=c(5,6,2,2),mfrow = c(1, 1))
plot(areas$prop.landscape2,areas$logAREA, xlab="Proportion of forest",ylab="Home range size (log ha)",cex.axis=3,cex.lab=3,pch=16,cex=3)
par(new=T)
#Plot regression line
areas<-areas[order(-areas$prediction),]
plot(areas[c(1,nrow(areas)),"prediction"],col="red",lwd=3,type="l",axes=F,xlab="",ylab="") #I SELECTED THE FIRST AND LAST OBSERVATION TO MAKE THIS LINE BECAUSE THE PREDICTIONS WERE NOT IN A STRAIGHT LINE. I DO NOT THINK THIS IS THE WAY TO DO IT SINCE I AM NOT INCLUDING THE RANDOM FACTOR IN THE PREDICTIONS
#plot(areas$prediction,col="red",lwd=3,type="l")

dev.off()
```

```{r}

#Check the model without outliers
areas.without.outliers<-subset(areas,areas$AREA<100000)

random_mod_6<-lme(log(AREA) ~ scale(log(prop.landscape2)) + scale(ELEVATION), random = ~1|PATCH_ID,method = "REML", data = areas.without.outliers)
#This is the best model, I will assess which variables I should retain
summary(random_mod_6) 

areas %>% 
  filter(AREA<100000) %>% 
  ggplot(aes(x=logprop.landscape2,y=AREA)) +
  geom_point() + 
  geom_smooth(method="lm")

#Checking assumptions. Extracting the fitted and residual values
residuals.mod<-resid(random_mod_6,type="normalized")
fitted.mod<-fitted(random_mod_6)

#Plot
plot(fitted.mod,residuals.mod)
#plot(areas.without.outliers$log.prop.landscape2,residuals.mod)
plot(areas.without.outliers$ELEVATION,residuals.mod)

#The residuals look normaly distributed
qqnorm(random_mod_2$residuals)
abline(0,1)

#Checking homogeneity of variance. SEEMS VERY GOOD
plot(random_mod_2)
```





#Rotation analysis of home range
# HW: prepare tables with results

```{r}
#Load raster
forest<-raster("Map layers/Aug27_15rasters/forest27")

#Read excel data base of patch id, patch size and % forest
freq_patches<-read.xlsx("Data bases/Frequencies patches.xlsx",sheet=1)

#Make an empty data base which will contain the information with NA's
metrics.or.rot <- data.frame("PATCH_ID"=freq_patches$PATCH,"FREQ"=freq_patches$FREQ, "PROP.FOR.OR"=NA, "PROP.FOR.ROT"=NA, "PATCH.SIZE.OR"=NA, "PATCH.SIZE.ROT"=NA, "CONNECTIVITY.OR"=NA, "CONNECTIVITY.ROT"=NA, "CLASS.COINC.PROB.ORG"=NA, "CLASS.COINC.PROB.ROT"=NA)
colnames(metrics.or.rot)<-c("PATCH_ID","FREQ","PROP.FOR.OR", "PROP.FOR.ROT", "PATCH.SIZE.OR", "PATCH.SIZE.ROT", "CONNECTIVITY.OR", "CONNECTIVITY.ROT", "CLASS.COINC.PROB.ORG", "CLASS.COINC.PROB.ROT")


```


#Step 2
```{r}
#Read functions. The only function here is the one that clips the original home range so that it can be rotated
source("Rotation home range functions.R")

#Obtain the frequencies from the isopleths data
frequencies<-as.numeric(substr(names(isopleths.birds),1,3))

#15 haspatch.cohesion.index of NA, repalce by 0

#Start loop for all frequencies
#for (i in c(1:11,13,15:17,19:21,23)){ #This row was active in the original code
for (i in 1:length(frequencies)){
  
# Clip forest grid to polygon
forest.clip <- clip(raster=forest, shape=isopleths.birds[[i]][5,])

#Run this cod if I want to see the original homerange with the forest raster as a background
jpeg(paste("Graphs/ID home_range_raster_map_id",frequencies[i],".jpg",sep=""),width=1000, height=1000, quality=100)

forest.clip
plot(forest.clip)
lines(isopleths.birds[[i]][5,])
scalebar(d=(extent(forest.clip)@xmax-extent(forest.clip)@xmin)/4,type="bar")

dev.off()


# Rotate and clip 
#Rotate polygon and get coordinates of the new polygon but rotated (specify angle in degrees). Here because it is the original home range, rotate is set to 0
HR.rotated <- elide(isopleths.birds[[i]][5,], rotate=0, 
                    center=isopleths.birds[[i]]@polygons[[5]]@labpt)

#Cuts the map for the new rotated home range
forest.rotated <- clip(forest, shape=HR.rotated)
  
#jpeg(paste("Graphs/",frequencies[i],"home_range_raster.jpg",sep=""),
#       width=1000, height=1000, quality=100)
#  plot(forest.rotated)  
#  lines(isopleths.birds[[1]][5,])
#  lines(HR.rotated, col="red")
#  dev.off()


#Statitiscs of rotated polygon
Res.rotated <- ClassStat(forest.rotated, cellsize=10)     # Within rotated homerange

#Create data frame for the results fo the rotation of the home range
rotated_analysis<-data.frame(matrix(nrow = 359, ncol = 5))
colnames(rotated_analysis)<-c("class","prop_forest","mean_patch_size","connectivity","class_coincidence_prob")

#Loop for rotating and getting the landscape statistics for all the rotated alternatives
for(angle in 1:359)
{
  #Rotate polygon
  HR.rotated <- elide(isopleths.birds[[i]][5,], rotate=angle,                      center=isopleths.birds[[i]]@polygons[[5]]@labpt)
  
#Cuts the map for the new rotated home range
  forest.rotated <- clip(forest, shape=HR.rotated)
  
  #Plots the rotated home range with the forest raster map as a bakcground
  #plot(forest.rotated)  
  #lines(isopleths.birds[[1]][5,])
  #lines(HR.rotated, col="red")
  
  #Statitiscs of rotated polygon
  Res.rotated <- ClassStat(forest.rotated, cellsize=10)     # Within rotated homerange
  
  #Bind results of rotated home ranges with previous data base. When there is no forest in the home range, when I try to subset the information, a data base is generated with NA. Therefore, for each landscape metric, I am replacing the NA with 0 if there was no forest in the landscape. 
  rotated_analysis$prop_forest[angle]<-ifelse(is.na(Res.rotated$prop.landscape[2])=="TRUE",0,Res.rotated$prop.landscape[2])
  rotated_analysis$mean_patch_size[angle]<-ifelse(is.na(Res.rotated$mean.patch.area[2])=="TRUE",0,Res.rotated$mean.patch.area[2])
  rotated_analysis$connectivity[angle]<-ifelse(is.na(Res.rotated$patch.cohesion.index[2])=="TRUE",0,Res.rotated$patch.cohesion.index[2])
  rotated_analysis$class_coincidence_prob[angle]<-ifelse(is.na(Res.rotated$effective.mesh.size[2]/Res.rotated$total.area[2])=="TRUE",0,Res.rotated$effective.mesh.size[2]/Res.rotated$total.area[2])
  
    print(paste(angle,"of 359 of bird #",i))
    
}

#Landscape metrics for original home range for the two landscape types
Res.original<-ClassStat(forest.clip, cellsize=10)       # Within homerange

#If the home range did not include forest, then I have to make a data frame with 0's and use it in the code above to obtain the forest statistics
Res.original.with.zeros<-subset(Res.original,Res.original$class==1)[1,]
Res.original.with.zeros[Res.original.with.zeros>0]<-0

#The value for forest in the data frame is 2; therefore, if I stry to subset the landscape metrics values and the data frame does not have forest values, it will have 0 rows. If this happens, then I will use the data frame with 0's, otherwise I can subset the information as usual
Res.original<-if(nrow(subset(Res.original,Res.original$class==2))==0){
  Res.original<-Res.original.with.zeros
}else{
  subset(Res.original,Res.original$class==2)
  
}

#Make a data frame with the landscape metrics of the original home range and that with the mean of the rotated alternatives
metrics.or.rot$PROP.FOR.OR[i]<-Res.original$prop.landscape

metrics.or.rot$PROP.FOR.ROT[i]<-mean(rotated_analysis$prop_forest,na.rm=T)

metrics.or.rot$PATCH.SIZE.OR[i]<-Res.original$mean.patch.area

metrics.or.rot$PATCH.SIZE.ROT[i]<-mean(rotated_analysis$mean_patch_size,na.rm=T)

metrics.or.rot$CONNECTIVITY.OR[i]<-ifelse(is.na(Res.original$patch.cohesion.index)=="TRUE",0,Res.original$patch.cohesion.index)

metrics.or.rot$CONNECTIVITY.ROT[i]<-mean(rotated_analysis$connectivity,na.rm=T)

#If effective mes size and total area are 0, then when I divide both values I get an NaN. Therefore, if that happens I replace the NaN by 0
metrics.or.rot$CLASS.COINC.PROB.ORG[i]<-ifelse(Res.original$effective.mesh.size==0 & Res.original$total.area==0,0,Res.original$effective.mesh.size/Res.original$total.area)

metrics.or.rot$CLASS.COINC.PROB.ROT[i]<-mean(rotated_analysis$class_coincidence_prob,na.rm=T)



print(paste(i,"of",nrow(metrics.or.rot)))

}
```



#Boxplot graphs ofe each landscape variable
```{r}

#load("Telemetry analysis home range.RData")
metrics.or.rot.original<-metrics.or.rot


#Boxplot with the values of the original and rotated home ranges landscape metrics
metrics.or.rot %>% 
  dplyr::select(-PATCH_ID,-FREQ) %>% 
  tidyr::gather(landscape.metric,value,PROP.FOR.OR:CLASS.COINC.PROB.ROT) %>% 
  mutate(home.range.class=if_else(substr(.$landscape.metric,nchar(.$landscape.metric)-1,nchar(.$landscape.metric))=="OR" | substr(.$landscape.metric,nchar(.$landscape.metric)-1,nchar(.$landscape.metric))=="RG" ,"Original","Rotated"),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PROP","Proportion of forest",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PATC","Patch size",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CONN","Connectivity index",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CLAS","Class coincidence probability index",landscape.metric)) %>% 
  ggplot(aes(x=home.range.class,y=value)) +
  geom_boxplot() +
  facet_wrap(~landscape.metric,scales="free") +
  xlab("Home range") +
  ylab("Value")


#Boxplot for the difference between original and rotated home ranges with the different landscape metrics
metrics.or.rot %>% 
  dplyr::select(-PATCH_ID,-FREQ) %>% 
  dplyr::mutate(PROP.FOR.DIFF=PROP.FOR.OR-PROP.FOR.ROT,
         PATCH.SIZE.DIFF=PATCH.SIZE.OR-PATCH.SIZE.ROT,
         CONNECTIVITY.DIFF=CONNECTIVITY.OR-CONNECTIVITY.ROT,
         CLASS.COINC.PROB.DIFF=CLASS.COINC.PROB.ORG-CLASS.COINC.PROB.ROT) %>% 
  tidyr::gather(landscape.metric,value,PROP.FOR.DIFF:CLASS.COINC.PROB.DIFF) %>% 
  dplyr::mutate(landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PROP","Proportion of forest",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PATC","Patch size",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CONN","Connectivity index",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CLAS","Class coincidence probability index",landscape.metric)) %>% 
  ggplot(aes(x=landscape.metric,y=value)) +
  geom_boxplot() +
  facet_wrap(~landscape.metric,scales="free") +
  xlab("Landscape metric") +
  ylab("Difference between original and rotated home range")


#Boxplot considering Landscape Probability index as the connectivity index
metrics.or.rot %>% 
  #dplyr::select(-PATCH_ID,-FREQ) %>% 
  dplyr::mutate(PROP.FOR.DIFF=PROP.FOR.OR-PROP.FOR.ROT,
         PATCH.SIZE.DIFF=PATCH.SIZE.OR-PATCH.SIZE.ROT,
         CLASS.COINC.PROB.DIFF=CLASS.COINC.PROB.ORG-CLASS.COINC.PROB.ROT) %>% 
  tidyr::gather(landscape.metric,value,PROP.FOR.DIFF:CLASS.COINC.PROB.DIFF) %>% 
  dplyr::mutate(landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PROP","Proportion of forest",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PATC","Patch size",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CLAS","Connectivity",landscape.metric)) %>% 
  mutate(landscape.metric=factor(landscape.metric,levels=c("Patch size","Proportion of forest","Connectivity"))) %>% 
  ggplot(aes(x=landscape.metric,y=value)) +
  geom_boxplot() +
  facet_wrap(~landscape.metric,scales="free") +
  xlab("Landscape metric") +
  ylab("Difference (original - rotated home range)") +
  geom_hline(yintercept=0,col="red",linetype="dashed")

ggsave("Graphs/Home range original rotated 3 landscape metrics.jpg")

```

####TEST FOR PROPORTION OF FOREST####

```{r}

#Perform the test
t.test(metrics.or.rot$PROP.FOR.OR,metrics.or.rot$PROP.FOR.ROT,paired=T)

#Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot$PROP.FOR.OR-metrics.or.rot$PROP.FOR.ROT

#Test normality of the data
hist(diff) #The data does not look normally distributed
library("car")
qqPlot(diff)
shapiro.test(diff)

#I have to save the data base in long format to do the levene test
metrics.or.rot.long<-metrics.or.rot %>% 
  dplyr::select(-PATCH_ID,-FREQ) %>% 
  tidyr::gather(landscape.metric,value,PROP.FOR.OR:CLASS.COINC.PROB.ROT) %>% 
  mutate(home.range.class=if_else(substr(.$landscape.metric,nchar(.$landscape.metric)-1,nchar(.$landscape.metric))=="OR" | substr(.$landscape.metric,nchar(.$landscape.metric)-1,nchar(.$landscape.metric))=="RG" ,"Original","Rotated"),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PROP","Proportion of forest",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="PATC","Patch size",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CONN","Connectivity index",landscape.metric),
         landscape.metric=if_else(substr(.$landscape.metric,1,4)=="CLAS","Class coincidence probability index",landscape.metric))


#Perform test for homogeneity of variance. 
leveneTest(subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Proportion of forest")$value~subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Proportion of forest")$home.range.class,las=2)

#Take out situations where the original and rotated home range had  values of 1 and 1 or 0 and 0 (meaning that they were either completely on the forest or on the matrix and they did not had a choice to select)
metrics.or.rot2<-metrics.or.rot %>% 
  filter(PROP.FOR.OR!=1 & PROP.FOR.ROT!=1 | 
           PROP.FOR.OR!=0 & PROP.FOR.ROT!=0) 
  t.test(metrics.or.rot2$PROP.FOR.OR,metrics.or.rot2$PROP.FOR.ROT,paired=T)
  
  #Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot2$PROP.FOR.OR-metrics.or.rot2$PROP.FOR.ROT

#Test normality of the data
hist(diff) #Still the difference is not normaly distributed
library("car")
qqPlot(diff)
shapiro.test(diff)



#I DO NOT KNOW IF IT IS OK TO PERFORM THE TEST ON THE ORIGINAL VARIABLES. Therefore, I will transform the original variables (square root of the arcsine) as I do in proportion for an ANOVA test

metrics.or.rot<-metrics.or.rot.original %>% 
  mutate(PROP.FOR.OR=sqrt(asin(PROP.FOR.OR)),
         PROP.FOR.ROT=sqrt(asin(PROP.FOR.ROT)))

#Perform the test
t.test(metrics.or.rot$PROP.FOR.OR,metrics.or.rot$PROP.FOR.ROT,paired=T)

#Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot$PROP.FOR.OR-metrics.or.rot$PROP.FOR.ROT

#Test normality of the data
hist(diff) #Still data is not normally distrbuted
library("car")
qqPlot(diff)
shapiro.test(diff)

#Perform a non parametric test
wilcox.test(metrics.or.rot.original$PROP.FOR.OR, metrics.or.rot.original$PROP.FOR.ROT, paired = TRUE, alternative = "two.sided") #RESULT: No differences

#Perform non parametric test without ties
metrics.or.rot.original.without.zeros<-metrics.or.rot.original %>% 
  mutate(diff=PROP.FOR.OR-PROP.FOR.ROT) %>% 
  filter(diff!=0)
wilcox.test(metrics.or.rot.original.without.zeros$PROP.FOR.OR,metrics.or.rot.original.without.zeros$PROP.FOR.ROT, paired = TRUE, alternative = "two.sided") #RESULT: No differences

```



####TEST FOR PATCH SIZE####

```{r}

#Perform the test
t.test(metrics.or.rot$PATCH.SIZE.OR,metrics.or.rot$PATCH.SIZE.ROT,paired=T) #RESULT: no differences

#Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot$PATCH.SIZE.OR-metrics.or.rot$PATCH.SIZE.ROT

#Test normality of the data
hist(diff) #Looks normally distributed
library("car")
qqPlot(diff)
shapiro.test(diff) #Test for normality confirms normal distribution


#Perform test for homogeneity of variance. 
leveneTest(subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Patch size")$value~subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Patch size")$home.range.class,las=2) #Variances are homogeneus

```


####TEST FOR CONNECTIVITY####

```{r}

#Perform the test
t.test(metrics.or.rot$CONNECTIVITY.OR,metrics.or.rot$CONNECTIVITY.ROT,paired=T)

#Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot$CONNECTIVITY.OR-metrics.or.rot$CONNECTIVITY.ROT

#Test normality of the data
hist(diff) #Looks like normally distributed
library("car")
qqPlot(diff)
shapiro.test(diff) #qq plot and shaprio test state that it is not normally distributed

#Perform test for homogeneity of variance. 
leveneTest(subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Connectivity index")$value~subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Connectivity index")$home.range.class,las=2)


#Perform a non parametric test
wilcox.test(metrics.or.rot.original$CONNECTIVITY.OR, metrics.or.rot.original$CONNECTIVITY.ROT, paired = TRUE, alternative = "two.sided")

#Non-parametric test without ties
metrics.or.rot.original.without.zeros<-metrics.or.rot.original %>% 
  mutate(diff=CONNECTIVITY.OR-CONNECTIVITY.ROT) %>% 
  filter(diff!=0)
wilcox.test(metrics.or.rot.original.without.zeros$CONNECTIVITY.OR,metrics.or.rot.original.without.zeros$CONNECTIVITY.ROT, paired = TRUE, alternative = "two.sided") #RESULT: no differences

```


####TEST FOR CLASS COINCIDENCE PROBABILITY WITH 0'S####

```{r}

#Perform the test
t.test(metrics.or.rot$CLASS.COINC.PROB.ORG,metrics.or.rot$CLASS.COINC.PROB.ROT,paired=T)

#Obtain the difference between treatments to test the assumptions
diff<-metrics.or.rot$CLASS.COINC.PROB.ORG-metrics.or.rot$CLASS.COINC.PROB.ROT

#Test normality of the data
hist(diff) #Distribution looks skewed
library("car")
qqPlot(diff)
shapiro.test(diff) #Test confirms that distibution is skewed


#Perform test for homogeneity of variance. 
leveneTest(subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Class coincidence probability index")$value~subset(metrics.or.rot.long,metrics.or.rot.long$landscape.metric=="Class coincidence probability index")$home.range.class,las=2)


#Perform a non parametric test
wilcox.test(metrics.or.rot.original$CLASS.COINC.PROB.ORG, metrics.or.rot.original$CLASS.COINC.PROB.ROT, paired = TRUE, alternative = "two.sided") #RESULT: no differences

```
